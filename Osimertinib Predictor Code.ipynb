{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from CSV files\n",
    "training_raw_df = pd.read_csv('target_train.csv')\n",
    "pharmacy_raw_df = pd.read_csv('rxclms_train.csv')\n",
    "med_raw_df = pd.read_csv('medclms_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pharmacy Data Processing\n",
    "\n",
    "# Replace values in the pharmacy dataframe\n",
    "pharmacy_raw_df.columns\n",
    "value_mapping = {\"N\": 0, \"Y\": 1}\n",
    "pharmacy_raw_df = pharmacy_raw_df.replace(value_mapping)\n",
    "\n",
    "# Extract the first part of 'therapy_id' to create a new 'id' column\n",
    "pharmacy_raw_df['id'] = pharmacy_raw_df['therapy_id'].str.split('-').str[0]\n",
    "\n",
    "# Fill NaN values with 0 and drop 'therapy_id' column\n",
    "pharmacy_raw_df = pharmacy_raw_df.fillna(0)\n",
    "pharmacy_raw_df = pharmacy_raw_df.drop(columns=['therapy_id'])\n",
    "\n",
    "# Replace values in the 'maint_ind' column\n",
    "value_mapping = {\"NONMAINT\": 0, \"MAINT\": 1}\n",
    "pharmacy_raw_df = pharmacy_raw_df.replace(value_mapping)\n",
    "\n",
    "# Define columns of interest in the pharmacy dataframe\n",
    "pharmacy_columns = ['id', \"pay_day_supply_cnt\", \"rx_cost\", \"tot_drug_cost_accum_amt\", \"reversal_ind\", \"mail_order_ind\",\n",
    "                    \"generic_ind\", \"maint_ind\", \"ddi_ind\", \"anticoag_ind\", \"diarrhea_treat_ind\", \"nausea_treat_ind\", \"seizure_treat_ind\"]\n",
    "\n",
    "# Group by 'id' and aggregate pharmacy data\n",
    "pharmacy_df = pharmacy_raw_df[pharmacy_columns].groupby('id').agg({\n",
    "    'pay_day_supply_cnt': 'mean',\n",
    "    'rx_cost': 'mean',\n",
    "    'tot_drug_cost_accum_amt': 'sum',\n",
    "    'reversal_ind': 'max',\n",
    "    'mail_order_ind': 'max',\n",
    "    'generic_ind': 'max',\n",
    "    'maint_ind': 'max',\n",
    "    'ddi_ind': 'max',\n",
    "    'anticoag_ind': 'max',\n",
    "    'diarrhea_treat_ind': 'max',\n",
    "    'nausea_treat_ind': 'max',\n",
    "    'seizure_treat_ind': 'max'\n",
    "})\n",
    "\n",
    "# Reset index and convert 'id' to string\n",
    "pharmacy_df.reset_index(inplace=True)\n",
    "pharmacy_df['id'] = pharmacy_df['id'].astype(str)\n",
    "training_raw_df['id'] = training_raw_df['id'].astype(str)\n",
    "\n",
    "# Replace values in the 'generic' column\n",
    "value_mapping = {\"GENERIC\": 0, \"BRANDED\": 1}\n",
    "pharmacy_df = pharmacy_df.replace(value_mapping)\n",
    "\n",
    "# Merge pharmacy data with training data\n",
    "therapy_pharm = training_raw_df.merge(pharmacy_df, on='id', how='left')\n",
    "therapy_pharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical Data Processing\n",
    "\n",
    "# Extract the first part of 'therapy_id' to create a new 'id' column and drop 'therapy_id'\n",
    "med_raw_df['id'] = med_raw_df['therapy_id'].str.split('-').str[0]\n",
    "med_raw_df = med_raw_df.drop(columns=['therapy_id'])\n",
    "\n",
    "# Define columns related to diagnosis\n",
    "med_raw_df.columns\n",
    "diag_columns = ['id', 'primary_diag_cd', 'diag_cd2', 'diag_cd3', 'diag_cd4', 'diag_cd5', 'diag_cd6', 'diag_cd7',\n",
    "                'diag_cd8', 'diag_cd9']\n",
    "diag_columns_wid = ['primary_diag_cd', 'diag_cd2', 'diag_cd3', 'diag_cd4', 'diag_cd5', 'diag_cd6', 'diag_cd7',\n",
    "                    'diag_cd8', 'diag_cd9']\n",
    "\n",
    "# Extract first letters of diagnosis codes in specific columns\n",
    "med_diag = med_raw_df[diag_columns]\n",
    "med_diag[diag_columns_wid] = med_diag[diag_columns_wid].apply(lambda x: x.str[0])\n",
    "med_diag\n",
    "\n",
    "# Update original dataframe with modified diagnosis columns\n",
    "med_raw_df[diag_columns] = med_diag\n",
    "\n",
    "# Define columns of interest in the medical dataframe\n",
    "med_columns = ['id', 'primary_diag_cd', 'diag_cd2', 'diag_cd3', 'diag_cd4', 'diag_cd5', 'diag_cd6', 'diag_cd7',\n",
    "               'diag_cd8', 'diag_cd9', 'ade_diagnosis', 'seizure_diagnosis', 'pain_diagnosis', 'fatigue_diagnosis',\n",
    "               'nausea_diagnosis', 'hyperglycemia_diagnosis', 'constipation_diagnosis', 'diarrhea_diagnosis']\n",
    "\n",
    "# Group by 'id' and aggregate medical data\n",
    "med_df = med_raw_df[med_columns].groupby('id').agg({\n",
    "    'primary_diag_cd': 'sum',\n",
    "    'diag_cd2': 'sum',\n",
    "    'diag_cd3': 'sum',\n",
    "    'diag_cd4': 'sum',\n",
    "    'diag_cd5': 'sum',\n",
    "    'diag_cd6': 'sum',\n",
    "    'diag_cd7': 'sum',\n",
    "    'diag_cd8': 'sum',\n",
    "    'diag_cd9': 'sum',\n",
    "    'ade_diagnosis': 'max',\n",
    "    'seizure_diagnosis': 'max',\n",
    "    'pain_diagnosis': 'max',\n",
    "    'fatigue_diagnosis': 'max',\n",
    "    'nausea_diagnosis': 'max',\n",
    "    'hyperglycemia_diagnosis': 'max',\n",
    "    'constipation_diagnosis': 'max',\n",
    "    'diarrhea_diagnosis': 'max'\n",
    "})\n",
    "\n",
    "# Reset index and convert 'id' to string\n",
    "med_df.reset_index(inplace=True)\n",
    "pharmacy_df['id'] = pharmacy_df['id'].astype(str)\n",
    "med_df['id'] = med_df['id'].astype(str)\n",
    "\n",
    "# Process and update diagnosis columns with sets of letters\n",
    "med_df[diag_columns_wid] = med_df[diag_columns_wid].apply(lambda x: x.apply(lambda y: set(str(y))))\n",
    "med_df\n",
    "\n",
    "# Extract letters from diagnosis columns\n",
    "diag_letters = med_df[diag_columns_wid]\n",
    "diag_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Diagnoses Letters and Concatenation\n",
    "\n",
    "# Copy the DataFrame for processing letters\n",
    "df = diag_letters.copy()\n",
    "\n",
    "# Create columns for each letter of the alphabet\n",
    "letters = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "# Set initial values for each letter\n",
    "for letter in letters:\n",
    "    df[letter] = 0\n",
    "\n",
    "# Iterate over each letter and update the corresponding column in the DataFrame\n",
    "for letter in letters:\n",
    "    for column in diag_letters.columns:\n",
    "        df[letter] = diag_letters.apply(lambda row: any(letter in cell for cell in row), axis=1).astype(int)\n",
    "\n",
    "# Drop the original diagnosis columns\n",
    "df = df.drop(columns=diag_columns_wid)\n",
    "print(df)\n",
    "\n",
    "# Concatenate the new DataFrame with the medical data DataFrame\n",
    "med_df = pd.concat([med_df, df], axis=1)\n",
    "med_df.drop(columns=diag_columns_wid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging and Preprocessing\n",
    "\n",
    "# Merge dataframes\n",
    "therapy_pharm_med = therapy_pharm.merge(med_df, on='id', how='left')\n",
    "raw_df = therapy_pharm_med.copy()\n",
    "\n",
    "# One-hot encode 'sex_cd'\n",
    "raw_df = pd.get_dummies(raw_df, columns=['sex_cd'], prefix='sex')\n",
    "\n",
    "# Duplicate removal\n",
    "dedup_df = raw_df.copy()\n",
    "\n",
    "# Count duplicates based on 'id'\n",
    "duplicate_counts = dedup_df['id'].value_counts()\n",
    "print(duplicate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing values\n",
    "\n",
    "# Copy the DataFrame to avoid modifying the original\n",
    "fill_na = raw_df.copy()\n",
    "\n",
    "# Define the mapping of values\n",
    "value_mapping = {np.nan: np.nan, 0.0: \"False\", 1.0: \"True\"}\n",
    "fill_na[[\"cms_disabled_ind\",\"cms_low_income_ind\"]] = fill_na[[\"cms_disabled_ind\",\"cms_low_income_ind\"]].replace(value_mapping)\n",
    "fill_na.fillna(np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values for 'est_age' with mean\n",
    "fill_na['est_age'].fillna(fill_na['est_age'].mean(), inplace=True)\n",
    "\n",
    "# Fill categorical variables with mode\n",
    "columns_to_fill_mode = ['cms_disabled_ind', 'cms_low_income_ind', 'sex_F', 'sex_M', 'ade_diagnosis', 'seizure_diagnosis',\n",
    "                         'pain_diagnosis', 'fatigue_diagnosis', 'nausea_diagnosis', 'hyperglycemia_diagnosis',\n",
    "                         'constipation_diagnosis', 'reversal_ind', 'mail_order_ind', 'generic_ind', 'maint_ind', 'ddi_ind',\n",
    "                         'anticoag_ind', 'diarrhea_treat_ind', 'nausea_treat_ind', 'seizure_treat_ind', 'ade_diagnosis',\n",
    "                         'seizure_diagnosis', 'diarrhea_diagnosis']\n",
    "\n",
    "for column in columns_to_fill_mode:\n",
    "    fill_na[column].fillna(fill_na[column].mode().iloc[0], inplace=True)\n",
    "\n",
    "# Fill numerical variable 'race_cd' with mean\n",
    "fill_na['race_cd'].fillna(fill_na['race_cd'].mean(), inplace=True)\n",
    "\n",
    "# Function to fill remaining columns with mean\n",
    "def fill_columns_with_mean(df):\n",
    "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for letter in letters:\n",
    "        column_name = letter\n",
    "        if column_name in df.columns:\n",
    "            column_mean = round(df[column_name].mean())\n",
    "            df[column_name].fillna(column_mean, inplace=True)\n",
    "\n",
    "fill_columns_with_mean(fill_na)\n",
    "\n",
    "# Check for remaining NaN values\n",
    "nan_count = fill_na.isna().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Copy the DataFrame for further modifications\n",
    "target_train = fill_na.copy()\n",
    "\n",
    "# Map categorical values to numerical values\n",
    "value_mapping = {\"False\": 0, \"True\": 1}\n",
    "target_train = target_train.replace(value_mapping)\n",
    "\n",
    "# Map binary values to 0 and 1\n",
    "value_mapping = {False: 0, True: 1}\n",
    "columns_to_map_binary = ['sex_F', 'sex_M']\n",
    "target_train[columns_to_map_binary] = target_train[columns_to_map_binary].replace(value_mapping)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_remove = ['tgt_ade_dc_ind', 'id', 'therapy_id', 'therapy_start_date', 'therapy_end_date']\n",
    "target_train = target_train.drop(columns=columns_to_remove)\n",
    "\n",
    "# Create binary columns for race\n",
    "modified_tt = target_train.copy()\n",
    "race_key = {0: 'Unknown', 1: 'White', 2: 'Black', 3: 'Other', 4: 'Asian', 5: 'Hispanic', 6: 'Native American'}\n",
    "\n",
    "# Create binary columns for each race and set them all to 0 initially\n",
    "for index, race in race_key.items():\n",
    "    modified_tt[race] = 0\n",
    "\n",
    "# Iterate through the DataFrame and set the appropriate race column to 1 based on 'race_cd'\n",
    "for index, row in modified_tt.iterrows():\n",
    "    race_cd = row['race_cd']\n",
    "    if race_cd in race_key:\n",
    "        modified_tt.at[index, race_key[race_cd]] = 1\n",
    "    else:\n",
    "        modified_tt.at[index, 'Unknown'] = 1\n",
    "\n",
    "# Drop the original 'race_cd' column\n",
    "modified_tt = modified_tt.drop(columns=['race_cd'])\n",
    "\n",
    "# Update the target_train DataFrame\n",
    "target_train = modified_tt.copy()\n",
    "\n",
    "# Log-transform numerical columns\n",
    "target_train['rx_cost'] = np.log10(target_train['rx_cost'] + 10)\n",
    "target_train['tot_drug_cost_accum_amt'] = np.log10(target_train['tot_drug_cost_accum_amt'] + 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "# Define features and target variable\n",
    "X = target_train.drop(columns=['tgt_ade_dc_ind', 'id', 'therapy_id', 'therapy_start_date', 'therapy_end_date'])\n",
    "y = target_train['tgt_ade_dc_ind']\n",
    "\n",
    "# Apply SelectKBest class to extract top 10 best features using chi-squared test\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=11)\n",
    "fit = bestfeatures.fit(X, y)\n",
    "\n",
    "# Get feature scores and names\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Specs', 'Score']\n",
    "\n",
    "# Print 10 best features\n",
    "print(featureScores.nlargest(11, 'Score'))\n",
    "\n",
    "# Select top 11 features\n",
    "features = featureScores.nlargest(11, 'Score')\n",
    "cols = features['Specs']\n",
    "X = X[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "\n",
    "# Initialize variables to store ROC curve data for alpha=0.01\n",
    "fpr_alpha_0_01, tpr_alpha_0_01, thresholds_alpha_0_01 = None, None, None\n",
    "\n",
    "# Load your data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of L2 regularization strengths (alphas) to test\n",
    "alphas = np.logspace(-5, 2, 8)\n",
    "\n",
    "# Create empty lists to store ROC AUC values for each alpha\n",
    "roc_auc_values = []\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Iterate over different L2 regularization strengths\n",
    "for alpha in alphas:\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the neural network\n",
    "    model_final = model.fit(X_train, y_train, epochs=15, batch_size=64, verbose=0,\n",
    "                            validation_data=(X_val, y_val), class_weight=class_weight_dict)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate and store ROC AUC\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    roc_auc_values.append(auc)\n",
    "\n",
    "    if alpha == 0.01:\n",
    "        # Calculate the fpr and tpr for the ROC curve\n",
    "        y_val_final = y_val\n",
    "        y_pred_final = y_pred\n",
    "        fpr_alpha_0_01, tpr_alpha_0_01, thresholds_alpha_0_01 = roc_curve(y_val, y_pred)\n",
    "\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_dir, 'model_alpha_0.01.keras')\n",
    "        model.save(model_filename)\n",
    "\n",
    "        # Save other relevant data\n",
    "        model_data = {\n",
    "            'alphas': alphas,\n",
    "            'roc_auc_values': roc_auc_values,\n",
    "            'fpr_alpha_0_01': fpr_alpha_0_01,\n",
    "            'tpr_alpha_0_01': tpr_alpha_0_01,\n",
    "            'thresholds_alpha_0_01': thresholds_alpha_0_01,\n",
    "        }\n",
    "        model_data_filename = os.path.join(model_dir, 'model_data_alpha_0.01.keras')\n",
    "        with open(model_data_filename, 'wb') as file:\n",
    "            pickle.dump(model_data, file)\n",
    "\n",
    "        # Print AUC and classification report\n",
    "        print(f\"AUC (alpha={alpha}): {auc}\")\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        print(classification_report(y_val, y_pred_binary))\n",
    "        \n",
    "        # Create a confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_val, y_pred_binary)\n",
    "        \n",
    "        # Visualize the confusion matrix\n",
    "        labels = ['Negative', 'Positive']  # Assuming binary classification (0 and 1)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate percentages from the confusion matrix\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "        total_samples = tn + fp + fn + tp\n",
    "\n",
    "        percentages = {\n",
    "            'True Positive': (tp / total_samples) * 100,\n",
    "            'False Positive': (fp / total_samples) * 100,\n",
    "            'True Negative': (tn / total_samples) * 100,\n",
    "            'False Negative': (fn / total_samples) * 100\n",
    "        }\n",
    "\n",
    "        print(\"Percentages:\")\n",
    "        for label, percentage in percentages.items():\n",
    "            print(f\"{label}: {percentage:.2f}%\")\n",
    "\n",
    "        if alpha == 0.01:\n",
    "            # Save confusion matrix for alpha=0.01\n",
    "            conf_matrix_alpha_0_01 = conf_matrix\n",
    "\n",
    "# Plot the ROC AUC values\n",
    "plt.plot(alphas, roc_auc_values, linestyle='-')\n",
    "plt.xlabel(\"L2 Regularization Strength (Alpha)\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.title(\"ROC AUC vs. L2 Regularization Strength\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot for alpha=0.01\n",
    "if fpr_alpha_0_01 is not None and tpr_alpha_0_01 is not None:\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_alpha_0_01, tpr_alpha_0_01)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (alpha=0.01)')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the ROC curve for alpha=0.01\n",
    "if fpr_alpha_0_01 is not None and tpr_alpha_0_01 is not None:\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_alpha_0_01, tpr_alpha_0_01, label='ROC Curve (alpha=0.01, AUC=0.9376)')\n",
    "    plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on New Data\n",
    "\n",
    "# Load the model for alpha=0.01\n",
    "model_filename = os.path.join(model_dir, 'model_alpha_0.01.keras')\n",
    "loaded_model = keras.models.load_model(model_filename)\n",
    "\n",
    "# Make predictions on the new data using the loaded model\n",
    "y_pred_new = loaded_model.predict(final_data)\n",
    "y_pred_prob_flat = y_pred_new.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': data['id'],\n",
    "    'SCORE': y_pred_prob_flat\n",
    "})\n",
    "results_df = results_df.sort_values(by='SCORE', ascending=False)\n",
    "results_df['RANK'] = range(1, len(results_df) + 1)\n",
    "print(results_df)\n",
    "results_df.style.format(\"{:.f}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "# Save binary results to CSV for additional analysis or visualization\n",
    "results_df_graphs = pd.DataFrame({\n",
    "    'id': data['id'],\n",
    "    'SCORE': (y_pred_new > 0.5).astype(int).flatten()\n",
    "})\n",
    "results_df_graphs.to_csv(\"results_df_graphs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
